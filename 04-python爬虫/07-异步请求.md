## 1. 介绍

如果我们使用 requests 库来爬取某个站点的话，发出一个请求之后，程序必须要等待网站返回响应之后才能接着运行，而在等待响应的过程中，整个爬虫程序是一直在等待的，实际上没有做任何的事情。对于这种情况我们有没有优化方案呢？

经典的计算机科学强调高效的算法，尽可能快地完成计算。但是很多网络程序的时间并不是消耗在计算上，而是在等待许多慢速的连接或者低频事件的发生。

如果它为每个动态的请求启动一个线程的话，随着并发请求数量的增加，它会在耗尽套接字之前，耗尽内存或者线程相关的资源。使用异步 I/O 可以避免这个的问题。这些程序暴露出一个新的挑战：如何高效的等待大量网络事件。一个现代的解决方案是异步 I/O。

## 2. 网络请求

python网络请求主要有4种：**单线程同步请求**、**多线程请求**、**多进程请求**、 **异步请求**

爬取网页地址：https://ssr4.scrape.center/ 

这个网站在内部实现返回响应的逻辑的时候特意加了 5 秒的延迟，也就是说如果我们用 requests 来爬取其中某个页面的话，至少需要 5 秒才能得到响应。

### 单线程同步请求

```python
import requests
import logging
import time
logging.basicConfig(level=logging.INFO,
                   format='%(asctime)s - %(levelname)s: %(message)s')
TOTAL_NUMBER = 10
BASE_URL = 'https://ssr4.scrape.center/detail/{id}'
start_time = time.time()
for id in range(1, TOTAL_NUMBER + 1):
   url = BASE_URL.format(id=id)
   logging.info('scraping %s', url)
   response = requests.get(url)
end_time = time.time()
logging.info('total time %s seconds', end_time - start_time)
```

运行结果

![image-20210625101655475](https://typora-1300715298.cos.ap-shanghai.myqcloud.com/uPic/image-20210625101655475.png)

仅仅只请求10个网页用了157s，前几天几十妙左右。

### 多线程请求

多进程就是利用 CPU 的多核优势，在同一时间并行地执行多个任务，可以大大提高执行效率。

```python
import requests
import logging
import time
import threading

logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s: %(message)s')
TOTAL_NUMBER = 10
BASE_URL = 'https://ssr4.scrape.center/detail/{id}'
start_time = time.time()

urls = [BASE_URL.format(id=id) for id in range(1, TOTAL_NUMBER)]
def get_html(url):
    logging.info('scraping %s thread %s :', url,
                 threading.current_thread().name)
    return requests.get(url)
def time_thread(start):
    logging.info('total time %s seconds', end_time - start)

for url in urls:
    t = threading.Thread(target=get_html, args=(url,))
    t.start()
t.join()
logging.info('total time %s seconds', time.time() - start_time)
```

运行结果：

![结果](https://typora-1300715298.cos.ap-shanghai.myqcloud.com/uPic/image-20210625112617037.png)

多线程用了35.8s

### 多进程请求

```python
import requests
import logging
import time
import multiprocessing

logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s: %(message)s')
TOTAL_NUMBER = 10
BASE_URL = 'https://ssr4.scrape.center/detail/{id}'
start_time = time.time()

urls = [BASE_URL.format(id=id) for id in range(1, TOTAL_NUMBER)]


def get_html(url):
    logging.info('scraping %s process %s :', url,
                 multiprocessing.current_process().name)
    return requests.get(url)


if __name__ == '__main__':
    start = time.time()
    for url in urls:
        t = multiprocessing.Process(target=get_html, args=(url,))
        t.start()
    t.join()
    logging.info('total time %s seconds', time.time() - start_time)

```

运行结果：

![image-20210625113307044](https://typora-1300715298.cos.ap-shanghai.myqcloud.com/uPic/image-20210625113307044.png)

这次只用了21s左右。

### 异步协程请求

```python
import asyncio
import aiohttp
import time

TOTAL_NUMBER = 10
BASE_URL = 'https://ssr4.scrape.center/detail/{id}'
start_time = time.time()

urls = [BASE_URL.format(id=id) for id in range(1, TOTAL_NUMBER)]


async def get(url):
    session = aiohttp.ClientSession()
    response = await session.get(url)
    await response.text()
    await session.close()
    return response


async def request(url):
    print('Waiting for', url)
    await get(url)

tasks = [asyncio.ensure_future(request(url)) for url in urls]
loop = asyncio.get_event_loop()
loop.run_until_complete(asyncio.wait(tasks))

print(time.time() - start_time)
```

运行结果

![多协程](https://typora-1300715298.cos.ap-shanghai.myqcloud.com/uPic/image-20210625114820070.png)

用了36s左右和多线程差不多，不过内存占用更少。

## 3. 异步协程

### 基本了解

在了解异步协程之前，我们首先得了解一些基础概念，如阻塞和非阻塞、同步和异步、多进程和协程。

### 阻塞

阻塞状态指程序未得到所需计算资源时被挂起的状态。程序在等待某个操作完成期间，自身无法继续处理其他的事情，则称该程序在该操作上是阻塞的。

常见的阻塞形式有：网络 I/O 阻塞、磁盘 I/O 阻塞、用户输入阻塞等。阻塞是无处不在的，包括 CPU 切换上下文时，所有的进程都无法真正处理事情，它们也会被阻塞。如果是多核 CPU 则正在执行上下文切换操作的核不可被利用。

### 非阻塞

程序在等待某操作过程中，自身不被阻塞，可以继续处理其他的事情，则称该程序在该操作上是非阻塞的。

非阻塞并不是在任何程序级别、任何情况下都可以存在的。仅当程序封装的级别可以囊括独立的子程序单元时，它才可能存在非阻塞状态。

非阻塞的存在是因为阻塞存在，正因为某个操作阻塞导致的耗时与效率低下，我们才要把它变成非阻塞的。

### 同步

不同程序单元为了完成某个任务，在执行过程中需靠某种通信方式以协调一致，我们称这些程序单元是同步执行的。

例如购物系统中更新商品库存，需要用“行锁”作为通信信号，让不同的更新请求强制排队顺序执行，那更新库存的操作是同步的。

简言之，同步意味着有序。

### 异步

为完成某个任务，不同程序单元之间过程中无需通信协调，也能完成任务的方式，不相关的程序单元之间可以是异步的。

例如，爬虫下载网页。调度程序调用下载程序后，即可调度其他任务，而无需与该下载任务保持通信以协调行为。不同网页的下载、保存等操作都是无关的，也无需相互通知协调。这些异步操作的完成时刻并不确定。

简言之，异步意味着无序。

### 协程

协程，英文叫作 Coroutine，又称微线程、纤程，协程是一种用户态的轻量级线程。

协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈。因此协程能保留上一次调用时的状态，即所有局部状态的一个特定组合，每次过程重入时，就相当于进入上一次调用的状态。

协程本质上是个单进程，协程相对于多进程来说，无需线程上下文切换的开销，无需原子操作锁定及同步的开销，编程模型也非常简单。

我们可以使用协程来实现异步操作，比如在网络爬虫场景下，我们发出一个请求之后，需要等待一定的时间才能得到响应，但其实在这个等待过程中，程序可以干许多其他的事情，等到响应得到之后才切换回来继续处理，这样可以充分利用 CPU 和其他资源，这就是协程的优势。

## 4. 协程基本使用

`asyncio` 模块最大特点就是，只存在一个线程，跟 JavaScript 一样。

由于只有一个线程，就不可能多个任务同时运行。asyncio 是"多任务合作"模式（cooperative multitasking），允许异步任务交出执行权给其他任务，等到其他任务完成，再收回执行权继续往下执行，这跟 JavaScript 也是一样的。

由于代码的执行权在多个任务之间交换，所以看上去好像多个任务同时运行，其实底层只有一个线程，多个任务分享运行时间。

表面上，这是一个不合理的设计，明明有多线程多进程的能力，为什么放着多余的 CPU 核心不用，而只用一个线程呢？但是就像前面说的，单线程简化了很多问题，使得代码逻辑变得简单，写法符合直觉。

![img](https://www.wangbase.com/blogimg/asset/201911/bg2019112004.jpg)

asyncio 模块在单线程上启动一个事件循环（event loop），时刻监听新进入循环的事件，加以处理，并不断重复这个过程，直到异步任务结束。事件循环的内部机制，可以参考 [JavaScript 的模型](https://wangdoc.com/javascript/async/general.html)，两者是一样的。

![img](https://www.wangbase.com/blogimg/asset/201911/bg2019112005.jpg)

### 4.1 asyncio API

下面介绍 `asyncio` 模块最主要的几个API。注意，必须使用 Python 3.7 或更高版本，早期的语法已经变了。

第一步，`import` 加载 `asyncio` 模块。

> ```javascript
> import asyncio
> ```

第二步，函数前面加上 `async` 关键字，就变成了 async 函数。这种函数最大特点是执行可以暂停，交出执行权。

> ```javascript
> async def main():
> ```

第三步，在 async 函数内部的异步任务前面，加上`await`命令。

> ```javascript
> await asyncio.sleep(1)
> ```

上面代码中，`asyncio.sleep(1)` 方法可以生成一个异步任务，休眠1秒钟然后结束。

执行引擎遇到`await`命令，就会在异步任务开始执行之后，暂停当前 async 函数的执行，把执行权交给其他任务。等到异步任务结束，再把执行权交回 async 函数，继续往下执行。

第四步，`async.run()` 方法加载 async 函数，启动事件循环。

> ```javascript
> asyncio.run(main())
> ```

上面代码中，`asyncio.run()` 在事件循环上监听 async 函数`main`的执行。等到 `main` 执行完了，事件循环才会终止。

### 4.2 async 函数的示例

下面是 async 函数的例子，新建一个脚本`async.py`，代码如下。

> ```javascript
> #!/usr/bin/env python3
> # countasync.py
> 
> import asyncio
> 
> async def count():
>     print("One")
>     await asyncio.sleep(1)
>     print("Two")
> 
> async def main():
>     await asyncio.gather(count(), count(), count())
> 
> if __name__ == "__main__":
>     import time
>     s = time.perf_counter()
>     asyncio.run(main())
>     elapsed = time.perf_counter() - s
>     print(f"{__file__} executed in {elapsed:0.2f} seconds.")
> ```

上面脚本中，在 async 函数`main`的里面，`asyncio.gather()` 方法将多个异步任务（三个 `count()`）包装成一个新的异步任务，必须等到内部的多个异步任务都执行结束，这个新的异步任务才会结束。

脚本的运行结果如下。

> ```bash
> $ python3 countasync.py
> One
> One
> One
> Two
> Two
> Two
> countasync.py executed in 1.01 seconds.
> ```

上面运行结果的原因是，三个 `count()` 依次执行，打印完 `One`，就休眠1秒钟，把执行权交给下一个 `count()`，所以先连续打印出三个 `One`。等到1秒钟休眠结束，执行权重新交回第一个 `count()`，开始执行 `await` 命令下一行的语句，所以会接着打印出三个`Two`。脚本总的运行时间是1秒。

作为对比，下面是这个例子的同步版本 `sync.py`。

> ```javascript
> #!/usr/bin/env python3
> # sync.py
> 
> import time
> 
> def count():
>     print("One")
>     time.sleep(1)
>     print("Two")
> 
> def main():
>     for _ in range(3):
>         count()
> 
> if __name__ == "__main__":
>     s = time.perf_counter()
>     main()
>     elapsed = time.perf_counter() - s
>     print(f"{__file__} executed in {elapsed:0.2f} seconds.")
> ```

上面脚本的运行结果如下。

> ```bash
> python3 countsync.py
> One
> Two
> One
> Two
> One
> Two
> countsync.py executed in 3.01 seconds.
> ```

上面运行结果的原因是，三个 `count()` 都是同步执行，必须等到前一个执行完，才能执行后一个。脚本总的运行时间是3秒。

