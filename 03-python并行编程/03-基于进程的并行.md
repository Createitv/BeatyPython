## 1. 介绍

`multiprocessing` 是Python标准库中的模块，实现了共享内存机制，也就是说，可以让运行在不同处理器核心的进程能读取共享内存。

`mpi4py` 库实现了消息传递的编程范例（设计模式）。简单来说，就是进程之间不靠任何共享信息来进行通讯（也叫做shared nothing），所有的交流都通过传递信息代替。

这方面与使用共享内存通讯，通过加锁或类似机制实现互斥的技术行成对比。在信息传递的代码中，进程通过 `send()` 和 `receive` 进行交流。

在Python多进程的官方文档中，明确指出 `multiprocessing` 模块要求，使用此模块的函数的main模块对子类来说必须是可导入的（ https://docs.python.org/3.3/library/multiprocessing.html ）。

`__main__` 在IDLE中并不是可以导入的，即使你在IDLE中将文件当做一个脚本来运行。为了能正确使用此模块，本章我们将在命令行使用下面的命令运行脚本：

```shell
python multiprocessing example.py
```

## 2. 多进程基本使用

“产生”（spawn）的意思是，由父进程创建子进程。父进程既可以在产生子进程之后继续异步执行，也可以暂停等待子进程创建完成之后再继续执行。Python的multiprocessing库通过以下几步创建进程：

1. 创建进程对象
2. 调用 `start()` 方法，开启进程的活动
3. 调用 `join()` 方法，在进程结束之前一直等待

### 2.1. 代码

下面的例子创建了5个进程，每一个进程都分配了 `foo(i)` 函数， `i` 表示进程的id：

```
# -*- coding: utf-8 -*-

import multiprocessing

def foo(i):
    print ('called function in process: %s' %i)
    return

if __name__ == '__main__':
    Process_jobs = []
    for i in range(5):
        p = multiprocessing.Process(target=foo, args=(i,))
        Process_jobs.append(p)
        p.start()
        p.join()
```

执行本例需要打开命令行，到文件 `spawn_a_process.py` （脚本名字）所在的目录下，然后输入下面的命令执行：

```
python spawn_a_process.py
```

我们会得到以下结果：

```
$ python process_2.py
called function in process: 0
called function in process: 1
called function in process: 2
called function in process: 3
called function in process: 4
```

### 2.2. 解释

按照本节前面提到的步骤，创建进程对象首先需要引入multiprocessing模块：

```
import multiprocessing
```

然后，我们在主程序中创建进程对象：

```
p = multiprocessing.Process(target=foo, args=(i,))
```

最后，我们调用 `start()` 方法启动：

```
p.start()
```

进程对象的时候需要分配一个函数，作为进程的执行任务，本例中，这个函数是 `foo()` 。我们可以用元组的形式给函数传递一些参数。最后，使用进程对象调用 `join()` 方法。

如果没有 `join()` ，主进程退出之后子进程会留在idle中，你必须手动杀死它们。

### 2.3. 了解更多

这是因为，子进程创建的时候需要导入包含目标函数的脚本。通过在 `__main__` 代码块中实例化进程对象，我们可以预防无限递归调用。最佳实践是在不同的脚本文件中定义目标函数，然后导入进来使用。所以上面的代码可以修改为：

```python
import multiprocessing
import target_function
if __name__ == '__main__':
    Process_jobs = []
    for i in range(5):
        p = multiprocessing.Process(target=target_function.function,args=(i,))
        Process_jobs.append(p)
        p.start()
        p.join()
```

`target_function.py` 的内容如下：

```
def function(i):
    print('called function in process: %s' %i)
    return
```

输出和上面一样。

## 3. 线程命名

命名进程的方法和前一章中介绍的命名线程差不多

下面的代码在主程序中创建了一个有名字的进程和一个没有名字的进程，目标函数都是 `foo()` 函数。

```python
# 命名一个进程
import multiprocessing
import time

def foo():
    name = multiprocessing.current_process().name
    print("Starting %s \n" % name)
    time.sleep(3)
    print("Exiting %s \n" % name)

if __name__ == '__main__':
    process_with_name = multiprocessing.Process(name='foo_process', target=foo)
    process_with_name.daemon = True  # 注意原代码有这一行，但是译者发现删掉这一行才能得到正确输出
    process_with_default_name = multiprocessing.Process(target=foo)
    process_with_name.start()
    process_with_default_name.start()
```

运行上面的代码，打开终端输入:

```
python naming_process.py
```

输出的结果如下：

```
$ python naming_process.py
Starting foo_process
Starting Process-2
Exiting foo_process
Exiting Process-2
```

### 3.2. 讨论

这个过程和命名线程很像。命名进程需要为进程对象提供 `name` 参数：

```
process_with_name = multiprocessing.Process(name='foo_process', target=foo)
```

在本例子中，进程的名字就是 `foo_function` 。如果子进程需要知道父进程的名字，可以使用以下声明：

```
name = multiprocessing.current_process().name
```

然后就能看见父进程的名字。

## 4. 后台守护进程

如果需要处理比较巨大的任务，又不需要人为干预，将其作为后台进程执行是个非常常用的编程模型。此进程又可以和其他进程并发执行。通过Python的multiprocessing模块的后台进程选项，我们可以让进程在后台运行。

```python
import multiprocessing
import time

def foo():
    name = multiprocessing.current_process().name
    print("Starting %s " % name)
    time.sleep(3)
    print("Exiting %s " % name)

if __name__ == '__main__':
    background_process = multiprocessing.Process(name='background_process', target=foo)
    background_process.daemon = True
    NO_background_process = multiprocessing.Process(name='NO_background_process', target=foo)
    NO_background_process.daemon = False
    background_process.start()
    NO_background_process.start() #注释掉这一句讲没有任何输出
```

运行结果：

```shell
$ python background_process.py
Starting NO_background_process
Exiting NO_background_process
```

为了在后台运行进程，我们设置 `daemon` 参数为 `True`

```
background_process.daemon = True
```

在非后台运行的进程会看到一个输出，后台运行的没有输出，后台运行进程在主进程结束之后会自动结束。

注意，后台进程不允许创建子进程。否则，当后台进程跟随父进程退出的时候，子进程会变成孤儿进程。另外，它们并不是Unix的守护进程或服务（daemons or services），所以当非后台进程退出，它们会被终结。

## 5.杀掉一个进程

我们可以使用 `terminate()` 方法立即杀死一个进程。另外，我们可以使用 `is_alive()` 方法来判断一个进程是否还存活。

### 5.1 代码

```python
import multiprocessing
import time

def foo():
        print('Starting function')
        time.sleep(0.1)
        print('Finished function')

if __name__ == '__main__':
        p = multiprocessing.Process(target=foo)
        print('Process before execution:', p, p.is_alive())
        p.start()
        print('Process running:', p, p.is_alive())
        p.terminate()
        print('Process terminated:', p, p.is_alive())
        p.join()
        print('Process joined:', p, p.is_alive())
        print('Process exit code:', p.exitcode)
```

运行结果：

![运行结果](https://typora-1300715298.cos.ap-shanghai.myqcloud.com/uPic/image-20210624113614033.png)

### 5.2 解释

我们创建了一个线程，然后用 `is_alive()` 方法监控它的声明周期。然后通过调用 `terminate()` 方法结束进程。

最后，我们通过读进程的 `ExitCode` 状态码（status code）验证进程已经结束， `ExitCode` 可能的值如下：

- == 0: 没有错误正常退出
- \> 0: 进程有错误，并以此状态码退出
- < 0: 进程被 `-1 *` 的信号杀死并以此作为 ExitCode 退出

在我们的例子中，输出的 `ExitCode` 是 `-15` 。负数表示子进程被数字为15的信号杀死。

## 6. 自定义进程

实现一个自定义的进程子类，需要以下三步：

- 定义 `Process` 的子类
- 覆盖 `__init__(self [,args])` 方法来添加额外的参数
- 覆盖 `run(self, [.args])` 方法来实现 `Process` 启动的时候执行的任务

创建 `Porcess` 子类之后，你可以创建它的实例并通过 `start()` 方法启动它，启动之后会运行 `run()` 方法。

### 6.1 代码

我们将使用子类的形式重写之前的例子：

```python
# -*- coding: utf-8 -*-
# 自定义子类进程
import multiprocessing


class MyProcess(multiprocessing.Process):
    def run(self):
        print("Called run method in process: %s" % self.name)
        return


if __name__ == "__main__":
    jobs = []
    for i in range(5):
        p = MyProcess()
        jobs.append(p)
        p.start()
        p.join()

```

输入以下命令运行脚本：

```
python subclass_process.py
```

![image-20210624114014461](https://typora-1300715298.cos.ap-shanghai.myqcloud.com/uPic/image-20210624114014461.png) 

### 6.2 解释

每一个继承了 `Process` 并重写了 `run()` 方法的子类都代表一个进程。此方法是进程的入口：

```python
class MyProcess(multiprocessing.Process):
    def run(self):
        print ('called run method in process: %s' % self.name)
        return
```

在主程序中，我们创建了一些 `MyProcess()` 的子类。当 `start()` 方法被调用的时候进程开始执行：

```
p = MyProcess()
p.start()
```

`join()` 命令可以让主进程等待其他进程结束最后退出

## 7. 进程间对象交换

并行应用常常需要在进程之间交换数据。Multiprocessing库有两个Communication Channel可以交换对象：队列(queue)和管道（pipe）。

![../_images/communication-channel.png](https://python-parallel-programmning-cookbook.readthedocs.io/zh_CN/latest/_images/communication-channel.png)

### 7.1 使用队列交换对象

我们可以通过队列数据结构来共享对象。

`Queue` 返回一个进程共享的队列，是线程安全的，也是进程安全的。任何可序列化的对象（Python通过 `pickable` 模块序列化对象）都可以通过它进行交换。

### 7.2 队列交换对象代码

我们将展示如何使用队列来实现生产者-消费者问题。 `Producer` 类生产item放到队列中，然后 `Consumer` 类从队列中移除它们。

```python
import multiprocessing
import random
import time

class Producer(multiprocessing.Process):
    def __init__(self, queue):
        multiprocessing.Process.__init__(self)
        self.queue = queue

    def run(self):
        for i in range(10):
            item = random.randint(0, 256)
            self.queue.put(item)
            print("Process Producer : item %d appended to queue %s" % (item, self.name))
            time.sleep(1)
            print("The size of queue is %s" % self.queue.qsize())

class Consumer(multiprocessing.Process):
    def __init__(self, queue):
        multiprocessing.Process.__init__(self)
        self.queue = queue

    def run(self):
        while True:
            if self.queue.empty():
                print("the queue is empty")
                break
            else:
                time.sleep(2)
                item = self.queue.get()
                print('Process Consumer : item %d popped from by %s \n' % (item, self.name))
                time.sleep(1)

if __name__ == '__main__':
    queue = multiprocessing.Queue()
    process_producer = Producer(queue)
    process_consumer = Consumer(queue)
    process_producer.start()
    process_consumer.start()
    process_producer.join()
    process_consumer.join()
```

运行结果：

```shell
Chapter 3>python using_queue.py
Process Producer : item 69 appended to queue producer-1
The size of queue is 1
Process Producer : item 168 appended to queue producer-1
The size of queue is 2
Process Consumer : item 69 popped from by consumer-2
Process Producer : item 235 appended to queue producer-1
The size of queue is 2
Process Producer : item 152 appended to queue producer-1
The size of queue is 3
Process Producer : item 213 appended to queue producer-1
Process Consumer : item 168 popped from by consumer-2
The size of queue is 3
Process Producer : item 35 appended to queue producer-1
The size of queue is 4
Process Producer : item 218 appended to queue producer-1
The size of queue is 5
Process Producer : item 175 appended to queue producer-1
Process Consumer : item 235 popped from by consumer-2
The size of queue is 5
Process Producer : item 140 appended to queue producer-1
The size of queue is 6
Process Producer : item 241 appended to queue producer-1
The size of queue is 7
Process Consumer : item 152 popped from by consumer-2
Process Consumer : item 213 popped from by consumer-2
Process Consumer : item 35 popped from by consumer-2
Process Consumer : item 218 popped from by consumer-2
Process Consumer : item 175 popped from by consumer-2
Process Consumer : item 140 popped from by consumer-2
Process Consumer : item 241 popped from by consumer-2
the queue is empty
```

### 7.3 解释

我们使用 `multiprocessing` 类在主程序中创建了 `Queue` 的实例：

```python
if __name__ == '__main__':
    queue = multiprocessing.Queue()
```

然后我们创建了两个进程，生产者和消费者， `Queue` 对象作为一个属性。

```python
process_producer = Producer(queue)
process_consumer = Consumer(queue)
```

生产者类负责使用 `put()` 方法放入10个item：

```python
for i in range(10):
    item = random.randint(0, 256)
    self.queue.put(item)
```

消费者进程负责使用 `get()` 方法从队列中移除item，并且确认队列是否为空，如果为空，就执行 `break` 跳出 `while` 循环：

```python
def run(self):
    while True:
        if self.queue.empty():
            print("the queue is empty")
            break
        else:
            time.sleep(2)
            item = self.queue.get()
            print('Process Consumer : item %d popped from by %s \n' % (item, self.name))
            time.sleep(1)
```

## 8. 进程池

多进程库提供了 `Pool` 类来实现简单的多进程任务。 `Pool` 类有以下方法：

- `apply()`: 直到得到结果之前一直阻塞。
- `apply_async()`: 这是 `apply()` 方法的一个变体，返回的是一个result对象。这是一个异步的操作，在所有的子类执行之前不会锁住主进程。
- `map()`: 这是内置的 `map()` 函数的并行版本。在得到结果之前一直阻塞，此方法将可迭代的数据的每一个元素作为进程池的一个任务来执行。
- `map_async()`: 这是 `map()` 方法的一个变体，返回一个result对象。如果指定了回调函数，回调函数应该是callable的，并且只接受一个参数。当result准备好时会自动调用回调函数（除非调用失败）。回调函数应该立即完成，否则，持有result的进程将被阻塞。

### 8.1 代码

下面的例子展示了如果通过进程池来执行一个并行应用。我们创建了有4个进程的进程池，然后使用 `map()` 方法进行一个简单的计算。

```python
import multiprocessing

def function_square(data):
    result = data*data
    return result

if __name__ == '__main__':
    inputs = list(range(100))
    pool = multiprocessing.Pool(processes=4)
    pool_outputs = pool.map(function_square, inputs)
    pool.close()
    pool.join()
    print ('Pool    :', pool_outputs)
```

计算的结果如下：

```sh
$ python poll.py
('Pool    :', [0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361, 400, 441, 484, 529, 576, 625, 676, 729, 784, 841, 900, 961, 1024, 1089, 1156, 1225, 1296, 1369, 1444, 1521, 1600, 1681, 1764, 1849, 1936, 2025, 2116, 2209, 2304, 2401, 2500, 2601, 2704, 2809, 2916, 3025, 3136, 3249, 3364, 3481, 3600, 3721, 3844, 3969, 4096, 4225, 4356, 4489, 4624, 4761, 4900, 5041, 5184, 5329, 5476, 5625, 5776, 5929, 6084, 6241, 6400, 6561, 6724, 6889, 7056, 7225, 7396, 7569, 7744, 7921, 8100, 8281, 8464, 8649, 8836, 9025, 9216, 9409, 9604, 9801])
```

### 8.2 解释

`multiprocessing.Pool` 方法在输入元素上应用 `function_square` 方法来执行简单的计算。并行的进程数量是4：

```
pool = multiprocessing.Pool(processes=4)
```

`pool.map` 方法将一些独立的任务提交给进程池：

```
pool_outputs = pool.map(function_square, inputs)
```

`input` 是一个从 `0` 到 `100` 的list：

```
inputs = list(range(100))
```

计算的结果存储在 `pool_outputs` 中。最后的结果打印出来：

```
print ('Pool    :', pool_outputs)
```

需要注意的是， `pool.map()` 方法的结果和Python内置的 `map()` 结果是相同的，不同的是 `pool.map()` 是通过多个并行进程计算的。

